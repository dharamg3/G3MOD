 .../ABI/testing/sysfs-kernel-mm-cleancache         |   11 +
 Documentation/ABI/testing/sysfs-kernel-mm-zcache   |   54 +
 Documentation/vm/cleancache.txt                    |  267 ++++
 drivers/staging/Kconfig                            |    2 +
 drivers/staging/Makefile                           |    2 +
 drivers/staging/zcache/Kconfig                     |   17 +
 drivers/staging/zcache/Makefile                    |    3 +
 drivers/staging/zcache/zcache_drv.c                | 1320 ++++++++++++++++++++
 drivers/staging/zcache/zcache_drv.h                |   93 ++
 drivers/staging/zram/Kconfig                       |    5 +
 drivers/staging/zram/Makefile                      |    3 +-
 drivers/staging/zram/xvmalloc.c                    |    8 +
 fs/btrfs/extent_io.c                               |    9 +
 fs/btrfs/super.c                                   |    2 +
 fs/buffer.c                                        |    5 +
 fs/ext3/super.c                                    |    2 +
 fs/ext4/super.c                                    |    2 +
 fs/mpage.c                                         |    7 +
 fs/ocfs2/super.c                                   |    2 +
 fs/super.c                                         |    3 +
 include/linux/cleancache.h                         |  118 ++
 include/linux/fs.h                                 |    5 +
 mm/Kconfig                                         |   22 +
 mm/Makefile                                        |    1 +
 mm/cleancache.c                                    |  258 ++++
 mm/filemap.c                                       |   11 +
 mm/truncate.c                                      |   10 +
 27 files changed, 2241 insertions(+), 1 deletions(-)

diff --git a/Documentation/ABI/testing/sysfs-kernel-mm-cleancache b/Documentation/ABI/testing/sysfs-kernel-mm-cleancache
new file mode 100644
index 0000000..98654c8
--- /dev/null
+++ b/Documentation/ABI/testing/sysfs-kernel-mm-cleancache
@@ -0,0 +1,11 @@
+What:		/sys/kernel/mm/cleancache/
+Date:		June 2010
+Contact:	Dan Magenheimer <dan.magenheimer@oracle.com>
+Description:
+		/sys/kernel/mm/cleancache/ contains a number of files which
+		record a count of various cleancache operations
+		(sum across all filesystems):
+			succ_gets
+			failed_gets
+			puts
+			flushes
diff --git a/Documentation/ABI/testing/sysfs-kernel-mm-zcache b/Documentation/ABI/testing/sysfs-kernel-mm-zcache
new file mode 100644
index 0000000..b589a06
--- /dev/null
+++ b/Documentation/ABI/testing/sysfs-kernel-mm-zcache
@@ -0,0 +1,54 @@
+What:		/sys/kernel/mm/zcache
+Date:		July 2010
+Contact:	Nitin Gupta <ngupta@vflare.org>
+Description:
+		/sys/kernel/mm/zcache directory contains compressed cache
+		statistics for each pool. A separate pool is created for
+		every mount instance of cleancache-aware filesystems.
+
+What:		/sys/kernel/mm/zcache/pool<id>/zero_pages
+Date:		July 2010
+Contact:	Nitin Gupta <ngupta@vflare.org>
+Description:
+		The zero_pages file is read-only and specifies number of zero
+		filled pages found in this pool.
+
+What:		/sys/kernel/mm/zcache/pool<id>/orig_data_size
+Date:		July 2010
+Contact:	Nitin Gupta <ngupta@vflare.org>
+Description:
+		The orig_data_size file is read-only and specifies uncompressed
+		size of data stored in this pool. This excludes zero-filled
+		pages (zero_pages) since no memory is allocated for them.
+		Unit: bytes
+
+What:		/sys/kernel/mm/zcache/pool<id>/compr_data_size
+Date:		July 2010
+Contact:	Nitin Gupta <ngupta@vflare.org>
+Description:
+		The compr_data_size file is read-only and specifies compressed
+		size of data stored in this pool. So, compression ratio can be
+		calculated using orig_data_size and this statistic.
+		Unit: bytes
+
+What:		/sys/kernel/mm/zcache/pool<id>/mem_used_total
+Date:		July 2010
+Contact:	Nitin Gupta <ngupta@vflare.org>
+Description:
+		The mem_used_total file is read-only and specifies the amount
+		of memory, including allocator fragmentation and metadata
+		overhead, allocated for this pool. So, allocator space
+		efficiency can be calculated using compr_data_size and this
+		statistic.
+		Unit: bytes
+
+What:		/sys/kernel/mm/zcache/pool<id>/memlimit
+Date:		July 2010
+Contact:	Nitin Gupta <ngupta@vflare.org>
+Description:
+		The memlimit file is read-write and specifies upper bound on
+		the compressed data size (compr_data_dize) stored in this pool.
+		The value specified is rounded down to nearest multiple of
+		PAGE_SIZE.
+		Unit: bytes
+
diff --git a/Documentation/vm/cleancache.txt b/Documentation/vm/cleancache.txt
new file mode 100644
index 0000000..e38e719
--- /dev/null
+++ b/Documentation/vm/cleancache.txt
@@ -0,0 +1,267 @@
+MOTIVATION
+
+Cleancache is a new optional feature provided by the VFS layer that
+potentially dramatically increases page cache effectiveness for
+many workloads in many environments at a negligible cost.
+
+Cleancache can be thought of as a page-granularity victim cache for clean
+pages that the kernel's pageframe replacement algorithm (PFRA) would like
+to keep around, but can't since there isn't enough memory.  So when the
+PFRA "evicts" a page, it first attempts to put it into a synchronous
+concurrency-safe page-oriented "pseudo-RAM" device (such as Xen's
+Transcendent Memory, aka "tmem", or in-kernel compressed memory, aka "zmem",
+or other RAM-like devices) which is not directly accessible or addressable
+by the kernel and is of unknown and possibly time-varying size.  And when a
+cleancache-enabled filesystem wishes to access a page in a file on disk,
+it first checks cleancache to see if it already contains it; if it does,
+the page is copied into the kernel and a disk access is avoided.
+
+FAQs are included below.
+
+IMPLEMENTATION OVERVIEW
+
+A cleancache "backend" that interfaces to this pseudo-RAM links itself
+to the kernel's cleancache "frontend" by calling cleancache_register_ops,
+passing a pointer to a cleancache_ops structure with funcs set appropriately.
+Note that cleancache_register_ops returns the previous settings so that
+chaining can be pefromed if desired. The functions provided must conform to
+certain semantics as follows:
+
+Most important, cleancache is "ephemeral".  Pages which are copied into
+cleancache have an indefinite lifetime which is completely unknowable
+by the kernel and so may or may not still be in cleancache at any later time.
+Thus, as its name implies, cleancache is not suitable for dirty pages.
+Cleancache has complete discretion over what pages to preserve and what
+pages to discard and when.
+
+Mounting a cleancache-enabled filesystem should call "init_fs" to obtain a
+pool id which, if positive, must be saved in the filesystem's superblock;
+a negative return value indicates failure.  A "put_page" will copy a
+(presumably about-to-be-evicted) page into cleancache and associate it with
+the pool id, a file key, and a page index into the file.  (The combination
+of a pool id, a file key, and an index is sometimes called a "handle".)
+A "get_page" will copy the page, if found, from cleancache into kernel memory.
+A "flush_page" will ensure the page no longer is present in cleancache;
+a "flush_inode" will flush all pages associated with the specified file;
+and, when a filesystem is unmounted, a "flush_fs" will flush all pages in
+all files specified by the given pool id and also surrender the pool id.
+
+An "init_shared_fs", like init_fs, obtains a pool id but tells cleancache
+to treat the pool as shared using a 128-bit UUID as a key.  On systems
+that may run multiple kernels (such as hard partitioned or virtualized
+systems) that may share a clustered filesystem, and where cleancache
+may be shared among those kernels, calls to init_shared_fs that specify the
+same UUID will receive the same pool id, thus allowing the pages to
+be shared.  Note that any security requirements must be imposed outside
+of the kernel (e.g. by "tools" that control cleancache).  Or a
+cleancache implementation can simply disable shared_init by always
+returning a negative value.
+
+If a get_page is successful on a non-shared pool, the page is flushed (thus
+making cleancache an "exclusive" cache).  On a shared pool, the page
+is NOT flushed on a successful get_page so that it remains accessible to
+other sharers.  The kernel is responsible for ensuring coherency between
+cleancache (shared or not), the page cache, and the filesystem, using
+cleancache flush operations as required.
+
+Note that cleancache must enforce put-put-get coherency and get-get
+coherency.  For the former, if two puts are made to the same handle but
+with different data, say AAA by the first put and BBB by the second, a
+subsequent get can never return the stale data (AAA).  For get-get coherency,
+if a get for a given handle fails, subsequent gets for that handle will
+never succeed unless preceded by a successful put with that handle.
+
+Last, cleancache provides no SMP serialization guarantees; if two
+different Linux threads are simultaneously putting and flushing a page
+with the same handle, the results are indeterminate.  Callers must
+lock the page to ensure serial behavior.
+
+CLEANCACHE PERFORMANCE METRICS
+
+Cleancache monitoring is done by sysfs files in the
+/sys/kernel/mm/cleancache directory.  The effectiveness of cleancache
+can be measured (across all filesystems) with:
+
+succ_gets	- number of gets that were successful
+failed_gets	- number of gets that failed
+puts		- number of puts attempted (all "succeed")
+flushes		- number of flushes attempted
+
+A backend implementatation may provide additional metrics.
+
+FAQ
+
+1) Where's the value? (Andrew Morton)
+
+Cleancache provides a significant performance benefit to many workloads
+in many environments with negligible overhead by improving the
+effectiveness of the pagecache.  Clean pagecache pages are
+saved in pseudo-RAM (RAM that is otherwise not directly addressable to
+the kernel); fetching those pages later avoids "refaults" and thus
+disk reads.
+
+Cleancache (and its sister code "frontswap") provide interfaces for
+a new pseudo-RAM memory type that conceptually lies between fast
+kernel-directly-addressable RAM and slower DMA/asynchronous devices.
+Disallowing direct kernel or userland reads/writes to this pseudo-RAM
+is ideal when data is transformed to a different form and size (such
+as with compression) or secretly moved (as might be useful for write-
+balancing for some RAM-like devices).  Evicted page-cache pages (and
+swap pages) are a great use for this kind of slower-than-RAM-but-much-
+faster-than-disk pseudo-RAM and the cleancache (and frontswap)
+"page-object-oriented" specification provides a nice way to read and
+write -- and indirectly "name" -- the pages.
+
+In the virtual case, the whole point of virtualization is to statistically
+multiplex physical resources across the varying demands of multiple
+virtual machines.  This is really hard to do with RAM and efforts to
+do it well with no kernel change have essentially failed (except in some
+well-publicized special-case workloads).  Cleancache -- and frontswap --
+with a fairly small impact on the kernel, provide a huge amount
+of flexibility for more dynamic, flexible RAM multiplexing.
+Specifically, the Xen Transcendent Memory backend allows otherwise
+"fallow" hypervisor-owned RAM to not only be "time-shared" between multiple
+virtual machines, but the pages can be compressed and deduplicated to
+optimize RAM utilization.  And when guest OS's are induced to surrender
+underutilized RAM (e.g. with "self-ballooning"), page cache pages
+are the first to go, and cleancache allows those pages to be
+saved and reclaimed if overall host system memory conditions allow.
+
+2) Why does cleancache have its sticky fingers so deep inside the
+   filesystems and VFS? (Andrew Morton and Christoph Hellwig)
+
+The core hooks for cleancache in VFS are in most cases a single line
+and the minimum set are placed precisely where needed to maintain
+coherency (via cleancache_flush operations) between cleancache,
+the page cache, and disk.  All hooks compile into nothingness if
+cleancache is config'ed off and turn into a function-pointer-
+compare-to-NULL if config'ed on but no backend claims the ops
+functions, or to a compare-struct-element-to-negative if a
+backend claims the ops functions but a filesystem doesn't enable
+cleancache.
+
+Some filesystems are built entirely on top of VFS and the hooks
+in VFS are sufficient, so don't require an "init_fs" hook; the
+initial implementation of cleancache didn't provide this hook.
+But for some filesystems (such as btrfs), the VFS hooks are
+incomplete and one or more hooks in fs-specific code are required.
+And for some other filesystems, such as tmpfs, cleancache may
+be counterproductive.  So it seemed prudent to require a filesystem
+to "opt in" to use cleancache, which requires adding a hook in
+each filesystem.  Not all filesystems are supported by cleancache
+only because they haven't been tested.  The existing set should
+be sufficient to validate the concept, the opt-in approach means
+that untested filesystems are not affected, and the hooks in the
+existing filesystems should make it very easy to add more
+filesystems in the future.
+
+The total impact of the hooks to existing fs and mm files is 43
+lines added (not counting comments and blank lines).
+
+3) Why not make cleancache asynchronous and batched so it can
+   more easily interface with real devices with DMA instead
+   of copying each individual page? (Minchan Kim)
+
+The one-page-at-a-time copy semantics simplifies the implementation
+on both the frontend and backend and also allows the backend to
+do fancy things on-the-fly like page compression and
+page deduplication.  And since the data is "gone" (copied into/out
+of the pageframe) before the cleancache get/put call returns,
+a great deal of race conditions and potential coherency issues
+are avoided.  While the interface seems odd for a "real device"
+or for real kernel-addressable RAM, it makes perfect sense for
+pseudo-RAM.
+
+4) Why is non-shared cleancache "exclusive"?  And where is the
+   page "flushed" after a "get"? (Minchan Kim)
+
+The main reason is to free up memory in pseudo-RAM and to avoid
+unnecessary cleancache_flush calls.  If you want inclusive,
+the page can be "put" immediately following the "get".  If
+put-after-get for inclusive becomes common, the interface could
+be easily extended to add a "get_no_flush" call.
+
+The flush is done by the cleancache backend implementation.
+
+5) What's the performance impact?
+
+Performance analysis has been presented at OLS'09 and LCA'10.
+Briefly, performance gains can be significant on most workloads,
+especially when memory pressure is high (e.g. when RAM is
+overcommitted in a virtual workload); and because the hooks are
+invoked primarily in place of or in addition to a disk read/write,
+overhead is negligible even in worst case workloads.  Basically
+cleancache replaces I/O with memory-copy-CPU-overhead; on older
+single-core systems with slow memory-copy speeds, cleancache
+has little value, but in newer multicore machines, especially
+consolidated/virtualized machines, it has great value.
+
+6) How do I add cleancache support for filesystem X? (Boaz Harrash)
+
+Filesystems that are well-behaved and conform to certain
+restrictions can utilize cleancache simply by making a call to
+cleancache_init_fs at mount time.  Unusual, misbehaving, or
+poorly layered filesystems must either add additional hooks
+and/or undergo extensive additional testing... or should just
+not enable the optional cleancache.
+
+Some points for a filesystem to consider:
+
+- The FS should be block-device-based (e.g. a ram-based FS such
+  as tmpfs should not enable cleancache)
+- To ensure coherency/correctness, the FS must ensure that all
+  file removal or truncation operations either go through VFS or
+  add hooks to do the equivalent cleancache "flush" operations
+- To ensure coherency/correctness, either inode numbers must
+  be unique across the lifetime of the on-disk file OR the
+  FS must provide an "encode_fh" function.
+- The FS must call the VFS superblock alloc and deactivate routines
+  or add hooks to do the equivalent cleancache calls done there.
+- To maximize performance, all pages fetched from the FS should
+  go through the do_mpag_readpage routine or the FS should add
+  hooks to do the equivalent (cf. btrfs)
+- Currently, the FS blocksize must be the same as PAGESIZE.  This
+  is not an architectural restriction, but no backends currently
+  support anything different.
+- A clustered FS should invoke the "shared_init_fs" cleancache
+  hook to get best performance for some backends.
+
+7) Why not use the KVA of the inode as the key? (Christoph Hellwig)
+
+If cleancache would use the inode virtual address instead of
+inode/filehandle, the pool id could be eliminated.  But, this
+won't work because cleancache retains pagecache data pages
+persistently even when the inode has been pruned from the
+inode unused list, and only flushes the data page if the file
+gets removed/truncated.  So if cleancache used the inode kva,
+there would be potential coherency issues if/when the inode
+kva is reused for a different file.  Alternately, if cleancache
+flushed the pages when the inode kva was freed, much of the value
+of cleancache would be lost because the cache of pages in cleanache
+is potentially much larger than the kernel pagecache and is most
+useful if the pages survive inode cache removal.
+
+8) Why is a global variable required?
+
+The cleancache_enabled flag is checked in all of the frequently-used
+cleancache hooks.  The alternative is a function call to check a static
+variable. Since cleancache is enabled dynamically at runtime, systems
+that don't enable cleancache would suffer thousands (possibly
+tens-of-thousands) of unnecessary function calls per second.  So the
+global variable allows cleancache to be enabled by default at compile
+time, but have insignificant performance impact when cleancache remains
+disabled at runtime.
+
+9) Does cleanache work with KVM?
+
+The memory model of KVM is sufficiently different that a cleancache
+backend may have little value for KVM.  This remains to be tested,
+especially in an overcommitted system.
+
+10) Does cleancache work in userspace?  It sounds useful for
+   memory hungry caches like web browsers.  (Jamie Lokier)
+
+No plans yet, though we agree it sounds useful, at least for
+apps that bypass the page cache (e.g. O_DIRECT).
+
+Last updated: Dan Magenheimer, September 2 2010
diff --git a/drivers/staging/Kconfig b/drivers/staging/Kconfig
index 5c8fcfc..eea3001 100644
--- a/drivers/staging/Kconfig
+++ b/drivers/staging/Kconfig
@@ -127,6 +127,8 @@ source "drivers/staging/cs5535_gpio/Kconfig"
 
 source "drivers/staging/zram/Kconfig"
 
+source "drivers/staging/zcache/Kconfig"
+
 source "drivers/staging/wlags49_h2/Kconfig"
 
 source "drivers/staging/wlags49_h25/Kconfig"
diff --git a/drivers/staging/Makefile b/drivers/staging/Makefile
index d538863..f7ab035 100644
--- a/drivers/staging/Makefile
+++ b/drivers/staging/Makefile
@@ -46,6 +46,8 @@ obj-$(CONFIG_DX_SEP)            += sep/
 obj-$(CONFIG_IIO)		+= iio/
 obj-$(CONFIG_CS5535_GPIO)	+= cs5535_gpio/
 obj-$(CONFIG_ZRAM)		+= zram/
+obj-$(CONFIG_XVMALLOC)		+= zram/
+obj-$(CONFIG_ZCACHE)		+= zcache/
 obj-$(CONFIG_WLAGS49_H2)	+= wlags49_h2/
 obj-$(CONFIG_WLAGS49_H25)	+= wlags49_h25/
 obj-$(CONFIG_SAMSUNG_LAPTOP)	+= samsung-laptop/
diff --git a/drivers/staging/zcache/Kconfig b/drivers/staging/zcache/Kconfig
new file mode 100644
index 0000000..24e994d
--- /dev/null
+++ b/drivers/staging/zcache/Kconfig
@@ -0,0 +1,17 @@
+config ZCACHE
+	bool "Page cache compression support"
+	depends on CLEANCACHE
+	select XVMALLOC
+	select LZO_COMPRESS
+	select LZO_DECOMPRESS
+	default n
+	help
+	  Compresses relatively unused page cache pages and stores them in
+	  memory itself. This increases effective memory size and can help
+	  reduce access to backing store device(s) which is typically much
+	  slower than access to main memory.
+
+	  Statistics are expoted through sysfs interface:
+	  /sys/kernel/mm/zcache/
+
+	  Project home: http://compcache.googlecode.com/
diff --git a/drivers/staging/zcache/Makefile b/drivers/staging/zcache/Makefile
new file mode 100644
index 0000000..29f66d9
--- /dev/null
+++ b/drivers/staging/zcache/Makefile
@@ -0,0 +1,3 @@
+zcache-y	:=	zcache_drv.o
+
+obj-$(CONFIG_ZCACHE)	+=	zcache.o
diff --git a/drivers/staging/zcache/zcache_drv.c b/drivers/staging/zcache/zcache_drv.c
new file mode 100644
index 0000000..52d32ad
--- /dev/null
+++ b/drivers/staging/zcache/zcache_drv.c
@@ -0,0 +1,1320 @@
+/*
+ * Page cache compression support
+ *
+ * Copyright (C) 2010  Nitin Gupta
+ * Released under the terms of GNU General Public License Version 2.0
+ *
+ * Project home: http://compcache.googlecode.com/
+ *
+ * Design:
+ * zcache is implemented using 'cleancache' which provides callbacks
+ * (cleancache_ops) for events such as when a page is evicted from the
+ * (uncompressed) page cache (cleancache_ops.put_page), when it is
+ * needed again (cleancache_ops.get_page), when it needs to be freed
+ * (cleancache_ops.flush_page) and so on.
+ *
+ * A page in zcache is identified with tuple <pool_id, inode_no, index>
+ *  - pool_id: For every cleancache aware filesystem mounted, zcache
+ * creates a separate pool (struct zcache_pool). This is container for
+ * all metadata/data structures needed to locate pages cached for this
+ * instance of mounted filesystem.
+ *  - inode_no: This represents a file/inode within this filesystem. An
+ * inode is represented using struct zcache_inode_rb within zcache which
+ * are arranged in a red-black tree indexed using inode_no.
+ *  - index: This represents page/index within an inode. Pages within an
+ * inode are arranged in a radix tree. So, each node of red-black tree
+ * above refers to a separate radix tree.
+ *
+ * Locking order:
+ * 1. zcache_inode_rb->tree_lock	(spin_lock)
+ * 2. zcache_pool->tree_lock		(rwlock_t: write)
+ *
+ * Nodes in an inode tree are reference counted and are freed when they
+ * do not any pages i.e. corresponding radix tree, maintaining list of
+ * pages associated with this inode, is empty.
+ */
+
+#define KMSG_COMPONENT "zcache"
+#define pr_fmt(fmt) KMSG_COMPONENT ": " fmt
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/cleancache.h>
+#include <linux/cpu.h>
+#include <linux/highmem.h>
+#include <linux/lzo.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/u64_stats_sync.h>
+
+#include "zcache_drv.h"
+
+static DEFINE_PER_CPU(unsigned char *, compress_buffer);
+static DEFINE_PER_CPU(unsigned char *, compress_workmem);
+
+/*
+ * For zero-filled pages, we directly insert 'index' value
+ * in corresponding radix node. These defines make sure we
+ * do not try to store NULL value in radix node (index can
+ * be 0) and that we do not use the lowest bit (which radix
+ * tree uses for its own purpose).
+ */
+#define ZCACHE_ZERO_PAGE_INDEX_SHIFT	2
+#define ZCACHE_ZERO_PAGE_MARK_BIT	(1 << 1)
+
+struct zcache *zcache;
+
+/*
+ * Individual percpu values can go negative but the sum across all CPUs
+ * must always be positive (we store various counts). So, return sum as
+ * unsigned value.
+ */
+static u64 zcache_get_stat(struct zcache_pool *zpool,
+		enum zcache_pool_stats_index idx)
+{
+	int cpu;
+	s64 val = 0;
+
+	for_each_possible_cpu(cpu) {
+		unsigned int start;
+		struct zcache_pool_stats_cpu *stats;
+
+		stats = per_cpu_ptr(zpool->stats, cpu);
+		do {
+			start = u64_stats_fetch_begin(&stats->syncp);
+			val += stats->count[idx];
+		} while (u64_stats_fetch_retry(&stats->syncp, start));
+	}
+
+	BUG_ON(val < 0);
+	return val;
+}
+
+static void zcache_add_stat(struct zcache_pool *zpool,
+		enum zcache_pool_stats_index idx, s64 val)
+{
+	struct zcache_pool_stats_cpu *stats;
+
+	preempt_disable();
+	stats = __this_cpu_ptr(zpool->stats);
+	u64_stats_update_begin(&stats->syncp);
+	stats->count[idx] += val;
+	u64_stats_update_end(&stats->syncp);
+	preempt_enable();
+}
+
+static void zcache_inc_stat(struct zcache_pool *zpool,
+		enum zcache_pool_stats_index idx)
+{
+	zcache_add_stat(zpool, idx, 1);
+}
+
+static void zcache_dec_stat(struct zcache_pool *zpool,
+		enum zcache_pool_stats_index idx)
+{
+	zcache_add_stat(zpool, idx, -1);
+}
+
+static void zcache_dec_pages(struct zcache_pool *zpool, int is_zero)
+{
+	enum zcache_pool_stats_index idx;
+
+	if (is_zero)
+		idx = ZPOOL_STAT_PAGES_ZERO;
+	else
+		idx = ZPOOL_STAT_PAGES_STORED;
+
+	zcache_dec_stat(zpool, idx);
+}
+
+static u64 zcache_get_memlimit(struct zcache_pool *zpool)
+{
+	u64 memlimit;
+	unsigned int start;
+
+	do {
+		start = read_seqbegin(&zpool->memlimit_lock);
+		memlimit = zpool->memlimit;
+	} while (read_seqretry(&zpool->memlimit_lock, start));
+
+	return memlimit;
+}
+
+static void zcache_set_memlimit(struct zcache_pool *zpool, u64 memlimit)
+{
+	write_seqlock(&zpool->memlimit_lock);
+	zpool->memlimit = memlimit;
+	write_sequnlock(&zpool->memlimit_lock);
+}
+
+static void zcache_dump_stats(struct zcache_pool *zpool)
+{
+	enum zcache_pool_stats_index idx;
+
+	pr_debug("Dumping statistics for pool: %p\n", zpool);
+	pr_debug("%llu ", zpool->memlimit);
+	for (idx = 0; idx < ZPOOL_STAT_NSTATS; idx++)
+		pr_debug("%llu ", zcache_get_stat(zpool, idx));
+	pr_debug("\n");
+}
+
+static void zcache_destroy_pool(struct zcache_pool *zpool)
+{
+	int i;
+
+	if (!zpool)
+		return;
+
+	spin_lock(&zcache->pool_lock);
+	zcache->num_pools--;
+	for (i = 0; i < MAX_ZCACHE_POOLS; i++)
+		if (zcache->pools[i] == zpool)
+			break;
+	zcache->pools[i] = NULL;
+	spin_unlock(&zcache->pool_lock);
+
+	if (!RB_EMPTY_ROOT(&zpool->inode_tree)) {
+		pr_warn("Memory leak detected. Freeing non-empty pool!\n");
+		zcache_dump_stats(zpool);
+	}
+
+	free_percpu(zpool->stats);
+	xv_destroy_pool(zpool->xv_pool);
+	kfree(zpool);
+}
+
+/*
+ * Allocate a new zcache pool and set default memlimit.
+ *
+ * Returns pool_id on success, negative error code otherwise.
+ */
+int zcache_create_pool(void)
+{
+	int ret;
+	u64 memlimit;
+	struct zcache_pool *zpool = NULL;
+
+	spin_lock(&zcache->pool_lock);
+	if (zcache->num_pools == MAX_ZCACHE_POOLS) {
+		spin_unlock(&zcache->pool_lock);
+		pr_info("Cannot create new pool (limit: %u)\n",
+					MAX_ZCACHE_POOLS);
+		ret = -EPERM;
+		goto out;
+	}
+	zcache->num_pools++;
+	spin_unlock(&zcache->pool_lock);
+
+	zpool = kzalloc(sizeof(*zpool), GFP_KERNEL);
+	if (!zpool) {
+		spin_lock(&zcache->pool_lock);
+		zcache->num_pools--;
+		spin_unlock(&zcache->pool_lock);
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	zpool->stats = alloc_percpu(struct zcache_pool_stats_cpu);
+	if (!zpool->stats) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	zpool->xv_pool = xv_create_pool();
+	if (!zpool->xv_pool) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	rwlock_init(&zpool->tree_lock);
+	seqlock_init(&zpool->memlimit_lock);
+	zpool->inode_tree = RB_ROOT;
+
+	memlimit = zcache_pool_default_memlimit_perc_ram *
+				((totalram_pages << PAGE_SHIFT) / 100);
+	memlimit &= PAGE_MASK;
+	zcache_set_memlimit(zpool, memlimit);
+
+	/* Add to pool list */
+	spin_lock(&zcache->pool_lock);
+	for (ret = 0; ret < MAX_ZCACHE_POOLS; ret++)
+		if (!zcache->pools[ret])
+			break;
+	zcache->pools[ret] = zpool;
+	spin_unlock(&zcache->pool_lock);
+
+out:
+	if (ret < 0)
+		zcache_destroy_pool(zpool);
+
+	return ret;
+}
+
+/*
+ * Allocate a new zcache node and insert it in given pool's
+ * inode_tree at location 'inode_no'.
+ *
+ * On success, returns newly allocated node and increments
+ * its refcount for caller. Returns NULL on failure.
+ */
+static struct zcache_inode_rb *zcache_inode_create(int pool_id,
+						ino_t inode_no)
+{
+	unsigned long flags;
+	struct rb_node *parent, **link;
+	struct zcache_inode_rb *new_znode;
+	struct zcache_pool *zpool = zcache->pools[pool_id];
+
+	/*
+	 * We can end up allocating multiple nodes due to racing
+	 * zcache_put_page(). But only one will be added to zpool
+	 * inode tree and rest will be freed.
+	 *
+	 * To avoid this possibility of redundant allocation, we
+	 * could do it inside zpool tree lock. However, that seems
+	 * more wasteful.
+	 */
+	new_znode = kzalloc(sizeof(*new_znode), GFP_NOWAIT);
+	if (unlikely(!new_znode))
+		return NULL;
+
+	INIT_RADIX_TREE(&new_znode->page_tree, GFP_NOWAIT);
+	spin_lock_init(&new_znode->tree_lock);
+	kref_init(&new_znode->refcount);
+	RB_CLEAR_NODE(&new_znode->rb_node);
+	new_znode->inode_no = inode_no;
+	new_znode->pool = zpool;
+
+	parent = NULL;
+	write_lock_irqsave(&zpool->tree_lock, flags);
+	link = &zpool->inode_tree.rb_node;
+	while (*link) {
+		struct zcache_inode_rb *znode;
+
+		znode = rb_entry(*link, struct zcache_inode_rb, rb_node);
+		parent = *link;
+
+		if (znode->inode_no > inode_no)
+			link = &(*link)->rb_left;
+		else if (znode->inode_no < inode_no)
+			link = &(*link)->rb_right;
+		else {
+			/*
+			 * New node added by racing zcache_put_page(). Free
+			 * this newly allocated node and use the existing one.
+			 */
+			kfree(new_znode);
+			new_znode = znode;
+			goto out;
+		}
+	}
+
+	rb_link_node(&new_znode->rb_node, parent, link);
+	rb_insert_color(&new_znode->rb_node, &zpool->inode_tree);
+
+out:
+	kref_get(&new_znode->refcount);
+	write_unlock_irqrestore(&zpool->tree_lock, flags);
+
+	return new_znode;
+}
+
+/*
+ * Called under zcache_inode_rb->tree_lock
+ */
+static int zcache_inode_is_empty(struct zcache_inode_rb *znode)
+{
+	return znode->page_tree.rnode == NULL;
+}
+
+/*
+ * kref_put callback for zcache node.
+ *
+ * The node must have been isolated already.
+ */
+static void zcache_inode_release(struct kref *kref)
+{
+	struct zcache_inode_rb *znode;
+
+	znode = container_of(kref, struct zcache_inode_rb, refcount);
+	BUG_ON(!zcache_inode_is_empty(znode));
+	kfree(znode);
+}
+
+/*
+ * Removes the given node from its inode_tree and drops corresponding
+ * refcount. Its called when someone removes the last page from a
+ * zcache node.
+ *
+ * Called under zcache_inode_rb->spin_lock
+ */
+static void zcache_inode_isolate(struct zcache_inode_rb *znode)
+{
+	unsigned long flags;
+	struct zcache_pool *zpool = znode->pool;
+
+	write_lock_irqsave(&zpool->tree_lock, flags);
+	/*
+	 * Someone can get reference on this node before we could
+	 * acquire write lock above. We want to remove it from its
+	 * inode_tree when only the caller and corresponding inode_tree
+	 * holds a reference to it. This ensures that a racing zcache
+	 * put will not end up adding a page to an isolated node and
+	 * thereby losing that memory.
+	 *
+	 */
+	if (atomic_read(&znode->refcount.refcount) == 2) {
+		rb_erase(&znode->rb_node, &znode->pool->inode_tree);
+		RB_CLEAR_NODE(&znode->rb_node);
+		kref_put(&znode->refcount, zcache_inode_release);
+	}
+	write_unlock_irqrestore(&zpool->tree_lock, flags);
+}
+
+/*
+ * Find inode in the given pool at location 'inode_no'.
+ *
+ * If found, return the node pointer and increment its reference
+ * count; NULL otherwise.
+ */
+static struct zcache_inode_rb *zcache_find_inode(struct zcache_pool *zpool,
+						ino_t inode_no)
+{
+	unsigned long flags;
+	struct rb_node *node;
+
+	read_lock_irqsave(&zpool->tree_lock, flags);
+	node = zpool->inode_tree.rb_node;
+	while (node) {
+		struct zcache_inode_rb *znode;
+
+		znode = rb_entry(node, struct zcache_inode_rb, rb_node);
+		if (znode->inode_no > inode_no)
+			node = node->rb_left;
+		else if (znode->inode_no < inode_no)
+			node = node->rb_right;
+		else {
+			/* found */
+			kref_get(&znode->refcount);
+			read_unlock_irqrestore(&zpool->tree_lock, flags);
+			return znode;
+		}
+	}
+	read_unlock_irqrestore(&zpool->tree_lock, flags);
+
+	/* not found */
+	return NULL;
+}
+
+static void zcache_handle_zero_page(struct page *page)
+{
+	void *user_mem;
+
+	user_mem = kmap_atomic(page, KM_USER0);
+	memset(user_mem, 0, PAGE_SIZE);
+	kunmap_atomic(user_mem, KM_USER0);
+
+	flush_dcache_page(page);
+}
+
+static int zcache_page_zero_filled(void *ptr)
+{
+	unsigned int pos;
+	unsigned long *page;
+
+	page = (unsigned long *)ptr;
+
+	for (pos = 0; pos != PAGE_SIZE / sizeof(*page); pos++) {
+		if (page[pos])
+			return 0;
+	}
+
+	return 1;
+}
+
+
+/*
+ * Identifies if the given radix node pointer actually refers
+ * to a zero-filled page.
+ */
+static int zcache_is_zero_page(void *ptr)
+{
+	return (unsigned long)(ptr) & ZCACHE_ZERO_PAGE_MARK_BIT;
+}
+
+/*
+ * Returns "pointer" value to be stored in radix node for
+ * zero-filled page at the given index.
+ */
+static void *zcache_index_to_ptr(unsigned long index)
+{
+	return (void *)((index << ZCACHE_ZERO_PAGE_INDEX_SHIFT) |
+				ZCACHE_ZERO_PAGE_MARK_BIT);
+}
+
+/*
+ * Encode <page, offset> as a single "pointer" value which is stored
+ * in corresponding radix node.
+ */
+static void *zcache_xv_location_to_ptr(struct page *page, u32 offset)
+{
+	unsigned long ptrval;
+
+	ptrval = page_to_pfn(page) << PAGE_SHIFT;
+	ptrval |= (offset & ~PAGE_MASK);
+
+	return (void *)ptrval;
+}
+
+/*
+ * Decode <page, offset> pair from "pointer" value returned from
+ * radix tree lookup.
+ */
+static void zcache_ptr_to_xv_location(void *ptr, struct page **page,
+				u32 *offset)
+{
+	unsigned long ptrval = (unsigned long)ptr;
+
+	*page = pfn_to_page(ptrval >> PAGE_SHIFT);
+	*offset = ptrval & ~PAGE_MASK;
+}
+
+/*
+ * Radix node contains "pointer" value which encode <page, offset>
+ * pair, locating the compressed object. Header of the object then
+ * contains corresponding 'index' value.
+ */
+static unsigned long zcache_ptr_to_index(void *ptr)
+{
+	u32 offset;
+	struct page *page;
+	unsigned long index;
+	struct zcache_objheader *zheader;
+
+	if (zcache_is_zero_page(ptr))
+		return (unsigned long)(ptr) >> ZCACHE_ZERO_PAGE_INDEX_SHIFT;
+
+	zcache_ptr_to_xv_location(ptr, &page, &offset);
+
+	zheader = kmap_atomic(page, KM_USER0) + offset;
+	index = zheader->index;
+	kunmap_atomic(zheader, KM_USER0);
+
+	return index;
+}
+
+void zcache_free_page(struct zcache_pool *zpool, void *ptr)
+{
+	int is_zero;
+	unsigned long flags;
+
+	if (unlikely(!ptr))
+		return;
+
+	is_zero = zcache_is_zero_page(ptr);
+	if (!is_zero) {
+		int clen;
+		void *obj;
+		u32 offset;
+		struct page *page;
+
+		zcache_ptr_to_xv_location(ptr, &page, &offset);
+		obj = kmap_atomic(page, KM_USER0) + offset;
+		clen = xv_get_object_size(obj) -
+				sizeof(struct zcache_objheader);
+		kunmap_atomic(obj, KM_USER0);
+
+		zcache_add_stat(zpool, ZPOOL_STAT_COMPR_SIZE, -clen);
+		local_irq_save(flags);
+		xv_free(zpool->xv_pool, page, offset);
+		local_irq_restore(flags);
+	}
+
+	zcache_dec_pages(zpool, is_zero);
+}
+
+
+/*
+ * Allocate memory for storing the given page and insert
+ * it in the given node's page tree at location 'index'.
+ * Parameter 'is_zero' specifies if the page is zero-filled.
+ *
+ * Returns 0 on success, negative error code on failure.
+ */
+static int zcache_store_page(struct zcache_inode_rb *znode,
+			pgoff_t index, struct page *page, int is_zero)
+{
+	int ret;
+	void *nodeptr;
+	size_t clen;
+	unsigned long flags;
+
+	u32 zoffset;
+	struct page *zpage;
+	unsigned char *zbuffer, *zworkmem;
+	unsigned char *src_data, *dest_data;
+
+	struct zcache_objheader *zheader;
+	struct zcache_pool *zpool = znode->pool;
+
+	if (is_zero) {
+		nodeptr = zcache_index_to_ptr(index);
+		goto out_store;
+	}
+
+	preempt_disable();
+	zbuffer = __get_cpu_var(compress_buffer);
+	zworkmem = __get_cpu_var(compress_workmem);
+	if (unlikely(!zbuffer || !zworkmem)) {
+		ret = -EFAULT;
+		preempt_enable();
+		goto out;
+	}
+
+	src_data = kmap_atomic(page, KM_USER0);
+	ret = lzo1x_1_compress(src_data, PAGE_SIZE, zbuffer, &clen, zworkmem);
+	kunmap_atomic(src_data, KM_USER0);
+
+	if (unlikely(ret != LZO_E_OK) || clen > zcache_max_page_size) {
+		ret = -EINVAL;
+		preempt_enable();
+		goto out;
+	}
+
+	local_irq_save(flags);
+	ret = xv_malloc(zpool->xv_pool, clen + sizeof(*zheader),
+			&zpage, &zoffset, GFP_NOWAIT);
+	local_irq_restore(flags);
+	if (unlikely(ret)) {
+		ret = -ENOMEM;
+		preempt_enable();
+		goto out;
+	}
+
+	dest_data = kmap_atomic(zpage, KM_USER0) + zoffset;
+
+	/* Store index value in header */
+	zheader = (struct zcache_objheader *)dest_data;
+	zheader->index = index;
+	dest_data += sizeof(*zheader);
+
+	memcpy(dest_data, zbuffer, clen);
+	kunmap_atomic(dest_data, KM_USER0);
+	preempt_enable();
+
+	nodeptr = zcache_xv_location_to_ptr(zpage, zoffset);
+
+out_store:
+	spin_lock_irqsave(&znode->tree_lock, flags);
+	ret = radix_tree_insert(&znode->page_tree, index, nodeptr);
+	if (unlikely(ret)) {
+		spin_unlock_irqrestore(&znode->tree_lock, flags);
+		if (!is_zero)
+			__free_page(zpage);
+		goto out;
+	}
+	if (is_zero) {
+		zcache_inc_stat(zpool, ZPOOL_STAT_PAGES_ZERO);
+	} else {
+		int delta = zcache_max_page_size - clen;
+		zcache_add_stat(zpool, ZPOOL_STAT_COMPR_SIZE, -delta);
+		zcache_inc_stat(zpool, ZPOOL_STAT_PAGES_STORED);
+		radix_tree_tag_set(&znode->page_tree, index,
+				ZCACHE_TAG_NONZERO_PAGE);
+	}
+
+	spin_unlock_irqrestore(&znode->tree_lock, flags);
+
+	ret = 0; /* success */
+
+out:
+	return ret;
+}
+
+/*
+ * Free 'pages_to_free' number of pages associated with the
+ * given zcache node. Actual number of pages freed might be
+ * less than this since the node might not contain enough
+ * pages.
+ *
+ * Called under zcache_inode_rb->tree_lock
+ * (Code adapted from brd.c)
+ */
+#define FREE_BATCH 16
+static void zcache_free_inode_pages(struct zcache_inode_rb *znode,
+				u32 pages_to_free, enum zcache_tag tag)
+{
+	int count;
+	unsigned long index = 0;
+	struct zcache_pool *zpool = znode->pool;
+
+	do {
+		int i;
+		void *objs[FREE_BATCH];
+
+		if (tag == ZCACHE_TAG_INVALID)
+			count = radix_tree_gang_lookup(&znode->page_tree,
+					objs, index, FREE_BATCH);
+		else
+			count = radix_tree_gang_lookup_tag(&znode->page_tree,
+					objs, index, FREE_BATCH, tag);
+
+		if (count > pages_to_free)
+			count = pages_to_free;
+
+		for (i = 0; i < count; i++) {
+			void *obj;
+			unsigned long index;
+
+			index = zcache_ptr_to_index(objs[i]);
+			obj = radix_tree_delete(&znode->page_tree, index);
+			BUG_ON(obj != objs[i]);
+			zcache_free_page(zpool, obj);
+		}
+
+		index++;
+		pages_to_free -= count;
+	} while (pages_to_free && (count == FREE_BATCH));
+}
+
+/*
+ * Returns number of pages stored in excess of currently
+ * set memlimit for the given pool.
+ */
+static u32 zcache_count_excess_pages(struct zcache_pool *zpool)
+{
+	u32 excess_pages, memlimit_pages, pages_stored;
+
+	memlimit_pages = zcache_get_memlimit(zpool) >> PAGE_SHIFT;
+	pages_stored = zcache_get_stat(zpool, ZPOOL_STAT_PAGES_STORED);
+	excess_pages = pages_stored > memlimit_pages ?
+			pages_stored - memlimit_pages : 0;
+
+	return excess_pages;
+}
+
+/*
+ * Free pages from this pool till we come within its memlimit.
+ *
+ * Currently, its called only when user sets memlimit lower than the
+ * number of pages currently stored in that pool. We select nodes in
+ * order of increasing inode number. This, in general, has no correlation
+ * with the order in which these are added. So, it is essentially random
+ * selection of nodes. Pages within a victim node node are freed in order
+ * of increasing index number.
+ *
+ * Automatic cache resizing and better page replacement policies will
+ * be implemented later.
+ */
+static void zcache_shrink_pool(struct zcache_pool *zpool)
+{
+	struct rb_node *node;
+	struct zcache_inode_rb *znode;
+
+	read_lock(&zpool->tree_lock);
+	node = rb_first(&zpool->inode_tree);
+	if (unlikely(!node)) {
+		read_unlock(&zpool->tree_lock);
+		return;
+	}
+	znode = rb_entry(node, struct zcache_inode_rb, rb_node);
+	kref_get(&znode->refcount);
+	read_unlock(&zpool->tree_lock);
+
+	do {
+		u32 pages_to_free;
+		struct rb_node *next_node;
+		struct zcache_inode_rb *next_znode;
+
+		pages_to_free = zcache_count_excess_pages(zpool);
+		if (!pages_to_free) {
+			spin_lock(&znode->tree_lock);
+			if (zcache_inode_is_empty(znode))
+				zcache_inode_isolate(znode);
+			spin_unlock(&znode->tree_lock);
+
+			kref_put(&znode->refcount, zcache_inode_release);
+			break;
+		}
+
+		/*
+		 * Get the next victim node before we (possibly) isolate
+		 * the current node.
+		 */
+		read_lock(&zpool->tree_lock);
+		next_node = rb_next(node);
+		next_znode = NULL;
+		if (next_node) {
+			next_znode = rb_entry(next_node,
+				struct zcache_inode_rb, rb_node);
+			kref_get(&next_znode->refcount);
+		}
+		read_unlock(&zpool->tree_lock);
+
+		spin_lock(&znode->tree_lock);
+		/* Free 'pages_to_free' non-zero pages in the current node */
+		zcache_free_inode_pages(znode, pages_to_free,
+					ZCACHE_TAG_NONZERO_PAGE);
+		if (zcache_inode_is_empty(znode))
+			zcache_inode_isolate(znode);
+		spin_unlock(&znode->tree_lock);
+
+		kref_put(&znode->refcount, zcache_inode_release);
+
+		/* Avoid busy-looping */
+		cond_resched();
+
+		node = next_node;
+		znode = next_znode;
+	} while (znode);
+}
+
+#ifdef CONFIG_SYSFS
+
+#define ZCACHE_POOL_ATTR_RO(_name) \
+	static struct kobj_attribute _name##_attr = __ATTR_RO(_name)
+
+#define ZCACHE_POOL_ATTR(_name) \
+	static struct kobj_attribute _name##_attr = \
+		__ATTR(_name, 0644, _name##_show, _name##_store)
+
+static struct zcache_pool *zcache_kobj_to_pool(struct kobject *kobj)
+{
+	int i;
+
+	spin_lock(&zcache->pool_lock);
+	for (i = 0; i < MAX_ZCACHE_POOLS; i++)
+		if (zcache->pools[i]->kobj == kobj)
+			break;
+	spin_unlock(&zcache->pool_lock);
+
+	return zcache->pools[i];
+}
+
+static ssize_t zero_pages_show(struct kobject *kobj,
+				struct kobj_attribute *attr, char *buf)
+{
+	struct zcache_pool *zpool = zcache_kobj_to_pool(kobj);
+
+	return sprintf(buf, "%llu\n", zcache_get_stat(
+			zpool, ZPOOL_STAT_PAGES_ZERO));
+}
+ZCACHE_POOL_ATTR_RO(zero_pages);
+
+static ssize_t orig_data_size_show(struct kobject *kobj,
+			       struct kobj_attribute *attr, char *buf)
+{
+	struct zcache_pool *zpool = zcache_kobj_to_pool(kobj);
+
+	return sprintf(buf, "%llu\n", zcache_get_stat(
+			zpool, ZPOOL_STAT_PAGES_STORED) << PAGE_SHIFT);
+}
+ZCACHE_POOL_ATTR_RO(orig_data_size);
+
+static ssize_t compr_data_size_show(struct kobject *kobj,
+			       struct kobj_attribute *attr, char *buf)
+{
+	struct zcache_pool *zpool = zcache_kobj_to_pool(kobj);
+
+	return sprintf(buf, "%llu\n", zcache_get_stat(
+			zpool, ZPOOL_STAT_COMPR_SIZE));
+}
+ZCACHE_POOL_ATTR_RO(compr_data_size);
+
+/*
+ * Total memory used by this pool, including allocator fragmentation
+ * and metadata overhead.
+ */
+static ssize_t mem_used_total_show(struct kobject *kobj,
+			       struct kobj_attribute *attr, char *buf)
+{
+	struct zcache_pool *zpool = zcache_kobj_to_pool(kobj);
+
+	return sprintf(buf, "%llu\n", xv_get_total_size_bytes(zpool->xv_pool));
+}
+ZCACHE_POOL_ATTR_RO(mem_used_total);
+
+static void memlimit_sysfs_common(struct kobject *kobj, u64 *value, int store)
+{
+	struct zcache_pool *zpool = zcache_kobj_to_pool(kobj);
+
+	if (store) {
+		zcache_set_memlimit(zpool, *value);
+		if (zcache_count_excess_pages(zpool))
+			zcache_shrink_pool(zpool);
+	} else {
+		*value = zcache_get_memlimit(zpool);
+	}
+}
+
+static ssize_t memlimit_store(struct kobject *kobj,
+		struct kobj_attribute *attr, const char *buf, size_t len)
+{
+	int ret;
+	u64 memlimit;
+
+	ret = strict_strtoull(buf, 10, &memlimit);
+	if (ret)
+		return ret;
+
+	memlimit &= PAGE_MASK;
+	memlimit_sysfs_common(kobj, &memlimit, 1);
+
+	return len;
+}
+
+static ssize_t memlimit_show(struct kobject *kobj,
+			struct kobj_attribute *attr, char *buf)
+{
+	u64 memlimit;
+
+	memlimit_sysfs_common(kobj, &memlimit, 0);
+	return sprintf(buf, "%llu\n", memlimit);
+}
+ZCACHE_POOL_ATTR(memlimit);
+
+static struct attribute *zcache_pool_attrs[] = {
+	&zero_pages_attr.attr,
+	&orig_data_size_attr.attr,
+	&compr_data_size_attr.attr,
+	&mem_used_total_attr.attr,
+	&memlimit_attr.attr,
+	NULL,
+};
+
+static struct attribute_group zcache_pool_attr_group = {
+	.attrs = zcache_pool_attrs,
+};
+#endif	/* CONFIG_SYSFS */
+
+
+/*
+ * cleancache_ops.init_fs
+ *
+ * Called whenever a cleancache aware filesystem is mounted.
+ * Creates and initializes a new zcache pool and inserts it
+ * in zcache pool list.
+ *
+ * Returns pool id on success, negative error code on failure.
+ */
+static int zcache_init_fs(size_t pagesize)
+{
+	int ret, pool_id;
+	struct zcache_pool *zpool = NULL;
+
+	/*
+	 * pagesize parameter probably makes sense only for Xen's
+	 * cleancache_ops provider which runs inside guests, passing
+	 * pages to the host. Since a guest might have a different
+	 * page size than that of host (really?), they need to pass
+	 * around this value.
+	 *
+	 * However, zcache runs on the host (or natively), so there
+	 * is no point of different page sizes.
+	 */
+	if (pagesize != PAGE_SIZE) {
+		pr_info("Unsupported page size: %zu", pagesize);
+		ret = -EINVAL;
+		goto out;
+	}
+
+	pool_id = zcache_create_pool();
+	if (pool_id < 0) {
+		pr_info("Failed to create new pool\n");
+		ret = -ENOMEM;
+		goto out;
+	}
+	zpool = zcache->pools[pool_id];
+
+#ifdef CONFIG_SYSFS
+	snprintf(zpool->name, MAX_ZPOOL_NAME_LEN, "pool%d", pool_id);
+
+	/* Create /sys/kernel/mm/zcache/pool<id> (<id> = 0, 1, ...) */
+	zpool->kobj = kobject_create_and_add(zpool->name, zcache->kobj);
+	if (!zpool->kobj) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	/* Create various nodes under /sys/.../pool<id>/ */
+	ret = sysfs_create_group(zpool->kobj, &zcache_pool_attr_group);
+	if (ret) {
+		kobject_put(zpool->kobj);
+		goto out;
+	}
+#endif
+
+	ret = pool_id;	/* success */
+
+out:
+	if (ret < 0)	/* failure */
+		zcache_destroy_pool(zpool);
+
+	return ret;
+}
+
+/*
+ * cleancache_ops.init_shared_fs
+ *
+ * Called whenever a cleancache aware clustered filesystem is mounted.
+ */
+static int zcache_init_shared_fs(char *uuid, size_t pagesize)
+{
+	/*
+	 * In Xen's implementation, cleancache_ops provider runs in each
+	 * guest, sending/receiving pages to/from the host. For each guest
+	 * that participates in ocfs2 like cluster, the client passes the
+	 * same uuid to the host. This allows the host to create a single
+	 * "shared pool" for all such guests to allow for feature like
+	 * data de-duplication among these guests.
+	 *
+	 * Whereas, zcache run directly on the host (or natively). So, for
+	 * any shared resource like ocfs2 mount point on host, it implicitly
+	 * creates a single pool. Thus, we can simply ignore this 'uuid' and
+	 * treat it like a usual filesystem.
+	 */
+	return zcache_init_fs(pagesize);
+}
+
+/*
+ * cleancache_ops.get_page
+ *
+ * Locates stored zcache page using <pool_id, inode_no, index>.
+ * If found, copies it to the given output page 'page' and frees
+ * zcache copy of the same.
+ *
+ * Returns 0 on success, negative error code on failure.
+ */
+static int zcache_get_page(int pool_id, struct cleancache_filekey filekey,
+			pgoff_t index, struct page *page)
+{
+	int ret;
+	void *nodeptr;
+	size_t clen;
+	ino_t inode_no = filekey.u.ino;
+	unsigned long flags;
+
+	u32 offset;
+	struct page *src_page;
+	unsigned char *src_data, *dest_data;
+
+	struct zcache_inode_rb *znode;
+	struct zcache_objheader *zheader;
+	struct zcache_pool *zpool = zcache->pools[pool_id];
+
+	znode = zcache_find_inode(zpool, inode_no);
+	if (!znode) {
+		ret = -EFAULT;
+		goto out;
+	}
+
+	BUG_ON(znode->inode_no != inode_no);
+
+	spin_lock_irqsave(&znode->tree_lock, flags);
+	nodeptr = radix_tree_delete(&znode->page_tree, index);
+	if (zcache_inode_is_empty(znode))
+		zcache_inode_isolate(znode);
+	spin_unlock_irqrestore(&znode->tree_lock, flags);
+
+	kref_put(&znode->refcount, zcache_inode_release);
+
+	if (!nodeptr) {
+		ret = -EFAULT;
+		goto out;
+	}
+
+	if (zcache_is_zero_page(nodeptr)) {
+		zcache_handle_zero_page(page);
+		goto out_free;
+	}
+
+	clen = PAGE_SIZE;
+	zcache_ptr_to_xv_location(nodeptr, &src_page, &offset);
+
+	src_data = kmap_atomic(src_page, KM_USER0) + offset;
+	zheader = (struct zcache_objheader *)src_data;
+	BUG_ON(zheader->index != index);
+
+	dest_data = kmap_atomic(page, KM_USER1);
+
+	ret = lzo1x_decompress_safe(src_data + sizeof(*zheader),
+			xv_get_object_size(src_data) - sizeof(*zheader),
+			dest_data, &clen);
+
+	kunmap_atomic(src_data, KM_USER0);
+	kunmap_atomic(dest_data, KM_USER1);
+
+	/* Failure here means bug in LZO! */
+	if (unlikely(ret != LZO_E_OK))
+		goto out_free;
+
+	flush_dcache_page(page);
+
+out_free:
+	zcache_free_page(zpool, nodeptr);
+	ret = 0; /* success */
+
+out:
+	return ret;
+}
+
+/*
+ * cleancache_ops.put_page
+ *
+ * Copies given input page 'page' to a newly allocated page.
+ * If allocation is successful, inserts it at zcache location
+ * <pool_id, inode_no, index>.
+ */
+static void zcache_put_page(int pool_id, struct cleancache_filekey filekey,
+			pgoff_t index, struct page *page)
+{
+	int ret, is_zero;
+	ino_t inode_no = filekey.u.ino;
+	unsigned long flags;
+	struct page *zpage;
+	struct zcache_inode_rb *znode;
+	struct zcache_pool *zpool = zcache->pools[pool_id];
+
+	/*
+	 * Check if the page is zero-filled. We do not allocate any
+	 * memory for such pages and hence they do not contribute
+	 * towards pool's memory usage. So, we can keep accepting
+	 * such pages even after we have reached memlimit.
+	 */
+	void *src_data = kmap_atomic(page, KM_USER0);
+	is_zero = zcache_page_zero_filled(src_data);
+	kunmap_atomic(src_data, KM_USER0);
+	if (is_zero)
+		goto out_find_store;
+
+	/*
+	 * Incrementing local compr_size before summing it from
+	 * all CPUs makes sure we do not end up storing pages in
+	 * excess of memlimit. In case of failure, we revert back
+	 * this local increment.
+	 */
+	zcache_add_stat(zpool, ZPOOL_STAT_COMPR_SIZE, zcache_max_page_size);
+
+	/*
+	 * memlimit can be changed any time by user using sysfs. If
+	 * it is set to a value smaller than current number of pages
+	 * stored, then excess pages are freed synchronously when this
+	 * sysfs event occurs.
+	 */
+	if (zcache_get_stat(zpool, ZPOOL_STAT_COMPR_SIZE) >
+			zcache_get_memlimit(zpool)) {
+		zcache_add_stat(zpool, ZPOOL_STAT_COMPR_SIZE,
+				-zcache_max_page_size);
+		return;
+	}
+
+out_find_store:
+	znode = zcache_find_inode(zpool, inode_no);
+	if (!znode) {
+		znode = zcache_inode_create(pool_id, inode_no);
+		if (unlikely(!znode)) {
+			zcache_add_stat(zpool, ZPOOL_STAT_COMPR_SIZE,
+					-zcache_max_page_size);
+			return;
+		}
+	}
+
+	/* Free page that might already be present at this index */
+	spin_lock_irqsave(&znode->tree_lock, flags);
+	zpage = radix_tree_delete(&znode->page_tree, index);
+	spin_unlock_irqrestore(&znode->tree_lock, flags);
+	if (zpage)
+		zcache_free_page(zpool, zpage);
+
+	ret = zcache_store_page(znode, index, page, is_zero);
+	if (unlikely(ret)) {
+		zcache_add_stat(zpool, ZPOOL_STAT_COMPR_SIZE,
+				-zcache_max_page_size);
+
+		/*
+		 * Its possible that racing zcache get/flush could not
+		 * isolate this node since we held a reference to it.
+		 */
+		spin_lock_irqsave(&znode->tree_lock, flags);
+		if (zcache_inode_is_empty(znode))
+			zcache_inode_isolate(znode);
+		spin_unlock_irqrestore(&znode->tree_lock, flags);
+	}
+
+	kref_put(&znode->refcount, zcache_inode_release);
+}
+
+/*
+ * cleancache_ops.flush_page
+ *
+ * Locates and fees page at zcache location <pool_id, inode_no, index>
+ */
+static void zcache_flush_page(int pool_id, struct cleancache_filekey filekey,
+			pgoff_t index)
+{
+	unsigned long flags;
+	ino_t inode_no = filekey.u.ino;
+	struct page *page;
+	struct zcache_inode_rb *znode;
+	struct zcache_pool *zpool = zcache->pools[pool_id];
+
+	znode = zcache_find_inode(zpool, inode_no);
+	if (!znode)
+		return;
+
+	spin_lock_irqsave(&znode->tree_lock, flags);
+	page = radix_tree_delete(&znode->page_tree, index);
+	if (zcache_inode_is_empty(znode))
+		zcache_inode_isolate(znode);
+	spin_unlock_irqrestore(&znode->tree_lock, flags);
+
+	kref_put(&znode->refcount, zcache_inode_release);
+	zcache_free_page(zpool, page);
+}
+
+/*
+ * cleancache_ops.flush_inode
+ *
+ * Free all pages associated with the given inode.
+ */
+static void zcache_flush_inode(int pool_id, struct cleancache_filekey filekey)
+{
+	unsigned long flags;
+	ino_t inode_no = filekey.u.ino;
+	struct zcache_pool *zpool;
+	struct zcache_inode_rb *znode;
+
+	zpool = zcache->pools[pool_id];
+
+	znode = zcache_find_inode(zpool, inode_no);
+	if (!znode)
+		return;
+
+	spin_lock_irqsave(&znode->tree_lock, flags);
+	zcache_free_inode_pages(znode, UINT_MAX, ZCACHE_TAG_INVALID);
+	if (zcache_inode_is_empty(znode))
+		zcache_inode_isolate(znode);
+	spin_unlock_irqrestore(&znode->tree_lock, flags);
+
+	kref_put(&znode->refcount, zcache_inode_release);
+}
+
+/*
+ * cleancache_ops.flush_fs
+ *
+ * Called whenever a cleancache aware filesystem is unmounted.
+ * Frees all metadata and data pages in corresponding zcache pool.
+ */
+static void zcache_flush_fs(int pool_id)
+{
+	struct rb_node *node;
+	struct zcache_inode_rb *znode;
+	struct zcache_pool *zpool = zcache->pools[pool_id];
+
+#ifdef CONFIG_SYSFS
+	/* Remove per-pool sysfs entries */
+	sysfs_remove_group(zpool->kobj, &zcache_pool_attr_group);
+	kobject_put(zpool->kobj);
+#endif
+
+	/*
+	 * At this point, there is no active I/O on this filesystem.
+	 * So we can free all its pages without holding any locks.
+	 */
+	node = rb_first(&zpool->inode_tree);
+	while (node) {
+		znode = rb_entry(node, struct zcache_inode_rb, rb_node);
+		node = rb_next(node);
+		zcache_free_inode_pages(znode, UINT_MAX, ZCACHE_TAG_INVALID);
+		rb_erase(&znode->rb_node, &zpool->inode_tree);
+		kfree(znode);
+	}
+
+	zcache_destroy_pool(zpool);
+}
+
+/*
+ * Callback for CPU hotplug events. Allocates percpu compression buffers.
+ */
+static int zcache_cpu_notify(struct notifier_block *nb, unsigned long action,
+			void *pcpu)
+{
+	int cpu = (long)pcpu;
+
+	switch (action) {
+	case CPU_UP_PREPARE:
+		per_cpu(compress_buffer, cpu) = (void *)__get_free_pages(
+					GFP_KERNEL | __GFP_ZERO, 1);
+		per_cpu(compress_workmem, cpu) = kzalloc(
+					LZO1X_MEM_COMPRESS, GFP_KERNEL);
+
+		break;
+	case CPU_DEAD:
+	case CPU_UP_CANCELED:
+		free_pages((unsigned long)(per_cpu(compress_buffer, cpu)), 1);
+		per_cpu(compress_buffer, cpu) = NULL;
+
+		kfree(per_cpu(compress_buffer, cpu));
+		per_cpu(compress_buffer, cpu) = NULL;
+
+		break;
+	default:
+		break;
+	}
+
+	return NOTIFY_OK;
+}
+
+static struct notifier_block zcache_cpu_nb = {
+	.notifier_call = zcache_cpu_notify
+};
+
+
+static int __init zcache_init(void)
+{
+	int ret = -ENOMEM;
+	unsigned int cpu;
+
+	struct cleancache_ops ops = {
+		.init_fs = zcache_init_fs,
+		.init_shared_fs = zcache_init_shared_fs,
+		.get_page = zcache_get_page,
+		.put_page = zcache_put_page,
+		.flush_page = zcache_flush_page,
+		.flush_inode = zcache_flush_inode,
+		.flush_fs = zcache_flush_fs,
+	};
+
+	zcache = kzalloc(sizeof(*zcache), GFP_KERNEL);
+	if (!zcache)
+		goto out;
+
+	ret = register_cpu_notifier(&zcache_cpu_nb);
+	if (ret)
+		goto out;
+
+	for_each_online_cpu(cpu) {
+		void *pcpu = (void *)(long)cpu;
+		zcache_cpu_notify(&zcache_cpu_nb, CPU_UP_PREPARE, pcpu);
+	}
+
+#ifdef CONFIG_SYSFS
+	/* Create /sys/kernel/mm/zcache/ */
+	zcache->kobj = kobject_create_and_add("zcache", mm_kobj);
+	if (!zcache->kobj)
+		goto out;
+#endif
+
+	spin_lock_init(&zcache->pool_lock);
+	cleancache_register_ops(&ops);
+
+	ret = 0; /* success */
+
+out:
+	if (ret)
+		kfree(zcache);
+
+	return 0;
+}
+
+module_init(zcache_init);
diff --git a/drivers/staging/zcache/zcache_drv.h b/drivers/staging/zcache/zcache_drv.h
new file mode 100644
index 0000000..9a512c3
--- /dev/null
+++ b/drivers/staging/zcache/zcache_drv.h
@@ -0,0 +1,93 @@
+/*
+ * Page cache compression support
+ *
+ * Copyright (C) 2010  Nitin Gupta
+ * Released under the terms of GNU General Public License Version 2.0
+ *
+ * Project home: http://compcache.googlecode.com/
+ */
+
+#ifndef _ZCACHE_DRV_H_
+#define _ZCACHE_DRV_H_
+
+#include <linux/kref.h>
+#include <linux/radix-tree.h>
+#include <linux/rbtree.h>
+#include <linux/rwlock.h>
+#include <linux/spinlock.h>
+#include <linux/seqlock.h>
+#include <linux/types.h>
+
+#include "../zram/xvmalloc.h"
+
+#define MAX_ZCACHE_POOLS	32	/* arbitrary */
+#define MAX_ZPOOL_NAME_LEN	8	/* "pool"+id (shown in sysfs) */
+
+enum zcache_pool_stats_index {
+	ZPOOL_STAT_PAGES_ZERO,
+	ZPOOL_STAT_PAGES_STORED,
+	ZPOOL_STAT_COMPR_SIZE,
+	ZPOOL_STAT_NSTATS,
+};
+
+/* Radix-tree tags */
+enum zcache_tag {
+	ZCACHE_TAG_NONZERO_PAGE,
+	ZCACHE_TAG_UNUSED,
+	ZCACHE_TAG_INVALID
+};
+
+/* Default zcache per-pool memlimit: 10% of total RAM */
+static const unsigned zcache_pool_default_memlimit_perc_ram = 10;
+
+/* We only keep pages that compress to less than this size */
+static const int zcache_max_page_size = PAGE_SIZE / 2;
+
+/* Stored in the beginning of each compressed object */
+struct zcache_objheader {
+	unsigned long index;
+};
+
+/* Red-Black tree node. Maps inode to its page-tree */
+struct zcache_inode_rb {
+	struct radix_tree_root page_tree; /* maps inode index to page */
+	spinlock_t tree_lock;		/* protects page_tree */
+	struct kref refcount;
+	struct rb_node rb_node;
+	ino_t inode_no;
+	struct zcache_pool *pool;	/* back-reference to parent pool */
+};
+
+struct zcache_pool_stats_cpu {
+	s64 count[ZPOOL_STAT_NSTATS];
+	struct u64_stats_sync syncp;
+};
+
+/* One zcache pool per (cleancache aware) filesystem mount instance */
+struct zcache_pool {
+	struct rb_root inode_tree;	/* maps inode number to page tree */
+	rwlock_t tree_lock;		/* protects inode_tree */
+
+	seqlock_t memlimit_lock;	/* protects memlimit */
+	u64 memlimit;			/* bytes */
+
+	struct xv_pool *xv_pool;	/* xvmalloc pool */
+	struct zcache_pool_stats_cpu *stats;	/* percpu stats */
+#ifdef CONFIG_SYSFS
+	unsigned char name[MAX_ZPOOL_NAME_LEN];
+	struct kobject *kobj;		/* sysfs */
+#endif
+};
+
+/* Manage all zcache pools */
+struct zcache {
+	struct zcache_pool *pools[MAX_ZCACHE_POOLS];
+	u32 num_pools;		/* current no. of zcache pools */
+	spinlock_t pool_lock;	/* protects pools[] and num_pools */
+#ifdef CONFIG_SYSFS
+	struct kobject *kobj;	/* sysfs */
+#endif
+
+};
+
+#endif
diff --git a/drivers/staging/zram/Kconfig b/drivers/staging/zram/Kconfig
index da079f8..1516ea4 100644
--- a/drivers/staging/zram/Kconfig
+++ b/drivers/staging/zram/Kconfig
@@ -1,6 +1,11 @@
+config XVMALLOC
+	bool
+	default n
+
 config ZRAM
 	tristate "Compressed RAM block device support"
 	depends on BLOCK
+	select XVMALLOC
 	select LZO_COMPRESS
 	select LZO_DECOMPRESS
 	default n
diff --git a/drivers/staging/zram/Makefile b/drivers/staging/zram/Makefile
index b1709c5..2a6d321 100644
--- a/drivers/staging/zram/Makefile
+++ b/drivers/staging/zram/Makefile
@@ -1,3 +1,4 @@
-zram-y	:=	zram_drv.o zram_sysfs.o xvmalloc.o
+zram-y	:=	zram_drv.o zram_sysfs.o
 
 obj-$(CONFIG_ZRAM)	+=	zram.o
+obj-$(CONFIG_XVMALLOC)	+=	xvmalloc.o
\ No newline at end of file
diff --git a/drivers/staging/zram/xvmalloc.c b/drivers/staging/zram/xvmalloc.c
index b644067..aa6fcd8 100644
--- a/drivers/staging/zram/xvmalloc.c
+++ b/drivers/staging/zram/xvmalloc.c
@@ -10,6 +10,8 @@
  * Released under the terms of GNU General Public License Version 2.0
  */
 
+#include <linux/module.h>
+#include <linux/kernel.h>
 #include <linux/bitops.h>
 #include <linux/errno.h>
 #include <linux/highmem.h>
@@ -320,11 +322,13 @@ struct xv_pool *xv_create_pool(void)
 
 	return pool;
 }
+EXPORT_SYMBOL_GPL(xv_create_pool);
 
 void xv_destroy_pool(struct xv_pool *pool)
 {
 	kfree(pool);
 }
+EXPORT_SYMBOL_GPL(xv_destroy_pool);
 
 /**
  * xv_malloc - Allocate block of given size from pool.
@@ -413,6 +417,7 @@ int xv_malloc(struct xv_pool *pool, u32 size, struct page **page,
 
 	return 0;
 }
+EXPORT_SYMBOL_GPL(xv_malloc);
 
 /*
  * Free block identified with <page, offset>
@@ -489,6 +494,7 @@ void xv_free(struct xv_pool *pool, struct page *page, u32 offset)
 	put_ptr_atomic(page_start, KM_USER0);
 	spin_unlock(&pool->lock);
 }
+EXPORT_SYMBOL_GPL(xv_free);
 
 u32 xv_get_object_size(void *obj)
 {
@@ -497,6 +503,7 @@ u32 xv_get_object_size(void *obj)
 	blk = (struct block_header *)((char *)(obj) - XV_ALIGN);
 	return blk->size;
 }
+EXPORT_SYMBOL_GPL(xv_get_object_size);
 
 /*
  * Returns total memory used by allocator (userdata + metadata)
@@ -505,3 +512,4 @@ u64 xv_get_total_size_bytes(struct xv_pool *pool)
 {
 	return pool->total_pages << PAGE_SHIFT;
 }
+EXPORT_SYMBOL_GPL(xv_get_total_size_bytes);
diff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c
index 2e993cf..2b890e1 100644
--- a/fs/btrfs/extent_io.c
+++ b/fs/btrfs/extent_io.c
@@ -10,6 +10,7 @@
 #include <linux/swap.h>
 #include <linux/writeback.h>
 #include <linux/pagevec.h>
+#include <linux/cleancache.h>
 #include "extent_io.h"
 #include "extent_map.h"
 #include "compat.h"
@@ -1981,6 +1982,13 @@ static int __extent_read_full_page(struct extent_io_tree *tree,
 
 	set_page_extent_mapped(page);
 
+	if (!PageUptodate(page)) {
+		if (cleancache_get_page(page) == 0) {
+			BUG_ON(blocksize != PAGE_SIZE);
+			goto out;
+		}
+	}
+
 	end = page_end;
 	while (1) {
 		lock_extent(tree, start, end, GFP_NOFS);
@@ -2108,6 +2116,7 @@ static int __extent_read_full_page(struct extent_io_tree *tree,
 		cur = cur + iosize;
 		page_offset += iosize;
 	}
+out:
 	if (!nr) {
 		if (!PageError(page))
 			SetPageUptodate(page);
diff --git a/fs/btrfs/super.c b/fs/btrfs/super.c
index b2130c4..15c757b 100644
--- a/fs/btrfs/super.c
+++ b/fs/btrfs/super.c
@@ -39,6 +39,7 @@
 #include <linux/miscdevice.h>
 #include <linux/magic.h>
 #include <linux/slab.h>
+#include <linux/cleancache.h>
 #include "compat.h"
 #include "ctree.h"
 #include "disk-io.h"
@@ -601,6 +602,7 @@ static int btrfs_fill_super(struct super_block *sb,
 	sb->s_root = root_dentry;
 
 	save_mount_options(sb, data);
+	cleancache_init_fs(sb);
 	return 0;
 
 fail_close:
diff --git a/fs/buffer.c b/fs/buffer.c
index 2219a76..65ad0b4 100644
--- a/fs/buffer.c
+++ b/fs/buffer.c
@@ -41,6 +41,7 @@
 #include <linux/bitops.h>
 #include <linux/mpage.h>
 #include <linux/bit_spinlock.h>
+#include <linux/cleancache.h>
 
 static int fsync_buffers_list(spinlock_t *lock, struct list_head *list);
 
@@ -277,6 +278,10 @@ void invalidate_bdev(struct block_device *bdev)
 	invalidate_bh_lrus();
 	lru_add_drain_all();	/* make sure all lru add caches are flushed */
 	invalidate_mapping_pages(mapping, 0, -1);
+	/* 99% of the time, we don't need to flush the cleancache on the bdev.
+	 * But, for the strange corners, lets be cautious
+	 */
+	cleancache_flush_inode(mapping);
 }
 EXPORT_SYMBOL(invalidate_bdev);
 
diff --git a/fs/ext3/super.c b/fs/ext3/super.c
index 7aa767d..dd2567a 100644
--- a/fs/ext3/super.c
+++ b/fs/ext3/super.c
@@ -36,6 +36,7 @@
 #include <linux/quotaops.h>
 #include <linux/seq_file.h>
 #include <linux/log2.h>
+#include <linux/cleancache.h>
 
 #include <asm/uaccess.h>
 
@@ -1367,6 +1368,7 @@ static int ext3_setup_super(struct super_block *sb, struct ext3_super_block *es,
 	} else {
 		ext3_msg(sb, KERN_INFO, "using internal journal");
 	}
+	cleancache_init_fs(sb);
 	return res;
 }
 
diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index cb10a06..01e0532 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -38,6 +38,7 @@
 #include <linux/ctype.h>
 #include <linux/log2.h>
 #include <linux/crc16.h>
+#include <linux/cleancache.h>
 #include <asm/uaccess.h>
 
 #include <linux/kthread.h>
@@ -1915,6 +1916,7 @@ static int ext4_setup_super(struct super_block *sb, struct ext4_super_block *es,
 			EXT4_INODES_PER_GROUP(sb),
 			sbi->s_mount_opt, sbi->s_mount_opt2);
 
+	cleancache_init_fs(sb);
 	return res;
 }
 
diff --git a/fs/mpage.c b/fs/mpage.c
index d78455a..7d0e666 100644
--- a/fs/mpage.c
+++ b/fs/mpage.c
@@ -27,6 +27,7 @@
 #include <linux/writeback.h>
 #include <linux/backing-dev.h>
 #include <linux/pagevec.h>
+#include <linux/cleancache.h>
 
 /*
  * I/O completion handler for multipage BIOs.
@@ -271,6 +272,12 @@ do_mpage_readpage(struct bio *bio, struct page *page, unsigned nr_pages,
 		SetPageMappedToDisk(page);
 	}
 
+	if (fully_mapped && blocks_per_page == 1 && !PageUptodate(page) &&
+	    cleancache_get_page(page) == 0) {
+		SetPageUptodate(page);
+		goto confused;
+	}
+
 	/*
 	 * This page will go to BIO.  Do we need to send this BIO off first?
 	 */
diff --git a/fs/ocfs2/super.c b/fs/ocfs2/super.c
index 06d1f74..88c19b2 100644
--- a/fs/ocfs2/super.c
+++ b/fs/ocfs2/super.c
@@ -41,6 +41,7 @@
 #include <linux/mount.h>
 #include <linux/seq_file.h>
 #include <linux/quotaops.h>
+#include <linux/cleancache.h>
 
 #define MLOG_MASK_PREFIX ML_SUPER
 #include <cluster/masklog.h>
@@ -2374,6 +2375,7 @@ static int ocfs2_initialize_super(struct super_block *sb,
 		mlog_errno(status);
 		goto bail;
 	}
+	cleancache_init_shared_fs((char *)&uuid_net_key, sb);
 
 bail:
 	mlog_exit(status);
diff --git a/fs/super.c b/fs/super.c
index 74e149e..30605f5 100644
--- a/fs/super.c
+++ b/fs/super.c
@@ -31,6 +31,7 @@
 #include <linux/mutex.h>
 #include <linux/backing-dev.h>
 #include <linux/rculist_bl.h>
+#include <linux/cleancache.h>
 #include "internal.h"
 
 
@@ -111,6 +112,7 @@ static struct super_block *alloc_super(struct file_system_type *type)
 		s->s_maxbytes = MAX_NON_LFS;
 		s->s_op = &default_op;
 		s->s_time_gran = 1000000000;
+		s->cleancache_poolid = -1;
 	}
 out:
 	return s;
@@ -177,6 +179,7 @@ void deactivate_locked_super(struct super_block *s)
 	struct file_system_type *fs = s->s_type;
 	if (atomic_dec_and_test(&s->s_active)) {
 		fs->kill_sb(s);
+		cleancache_flush_fs(s);
 		put_filesystem(fs);
 		put_super(s);
 	} else {
diff --git a/include/linux/cleancache.h b/include/linux/cleancache.h
new file mode 100644
index 0000000..0a4d0aa
--- /dev/null
+++ b/include/linux/cleancache.h
@@ -0,0 +1,118 @@
+#ifndef _LINUX_CLEANCACHE_H
+#define _LINUX_CLEANCACHE_H
+
+#include <linux/fs.h>
+#include <linux/exportfs.h>
+#include <linux/mm.h>
+
+#define CLEANCACHE_KEY_MAX 6
+
+/*
+ * cleancache requires every file with a page in cleancache to have a
+ * unique key unless/until the file is removed/truncated.  For some
+ * filesystems, the inode number is unique, but for "modern" filesystems
+ * an exportable filehandle is required (see exportfs.h)
+ */
+struct cleancache_filekey {
+	union {
+		ino_t ino;
+		__u32 fh[CLEANCACHE_KEY_MAX];
+		u32 key[CLEANCACHE_KEY_MAX];
+	} u;
+};
+
+struct cleancache_ops {
+	int (*init_fs)(size_t);
+	int (*init_shared_fs)(char *uuid, size_t);
+	int (*get_page)(int, struct cleancache_filekey,
+			pgoff_t, struct page *);
+	void (*put_page)(int, struct cleancache_filekey,
+			pgoff_t, struct page *);
+	void (*flush_page)(int, struct cleancache_filekey, pgoff_t);
+	void (*flush_inode)(int, struct cleancache_filekey);
+	void (*flush_fs)(int);
+};
+
+extern struct cleancache_ops
+	cleancache_register_ops(struct cleancache_ops *ops);
+extern void __cleancache_init_fs(struct super_block *);
+extern void __cleancache_init_shared_fs(char *, struct super_block *);
+extern int  __cleancache_get_page(struct page *);
+extern void __cleancache_put_page(struct page *);
+extern void __cleancache_flush_page(struct address_space *, struct page *);
+extern void __cleancache_flush_inode(struct address_space *);
+extern void __cleancache_flush_fs(struct super_block *);
+extern int cleancache_enabled;
+
+#ifdef CONFIG_CLEANCACHE
+#define cleancache_fs_enabled(_page) \
+	(_page->mapping->host->i_sb->cleancache_poolid >= 0)
+#define cleancache_fs_enabled_mapping(_mapping) \
+	(mapping->host->i_sb->cleancache_poolid >= 0)
+#else
+#define cleancache_enabled (0)
+#define cleancache_fs_enabled(_page) (0)
+#define cleancache_fs_enabled_mapping(_page) (0)
+#endif
+
+/*
+ * The shim layer provided by these inline functions allows the compiler
+ * to reduce all cleancache hooks to nothingness if CONFIG_CLEANCACHE
+ * is disabled, to a single global variable check if CONFIG_CLEANCACHE
+ * is enabled but no cleancache "backend" has dynamically enabled it,
+ * and, for the most frequent cleancache ops, to a single global variable
+ * check plus a superblock element comparison if CONFIG_CLEANCACHE is enabled
+ * and a cleancache backend has dynamically enabled cleancache, but the
+ * filesystem referenced by that cleancache op has not enabled cleancache.
+ * As a result, CONFIG_CLEANCACHE can be enabled by default with essentially
+ * no measurable performance impact.
+ */
+
+static inline void cleancache_init_fs(struct super_block *sb)
+{
+	if (cleancache_enabled)
+		__cleancache_init_fs(sb);
+}
+
+static inline void cleancache_init_shared_fs(char *uuid, struct super_block *sb)
+{
+	if (cleancache_enabled)
+		__cleancache_init_shared_fs(uuid, sb);
+}
+
+static inline int cleancache_get_page(struct page *page)
+{
+	int ret = -1;
+
+	if (cleancache_enabled && cleancache_fs_enabled(page))
+		ret = __cleancache_get_page(page);
+	return ret;
+}
+
+static inline void cleancache_put_page(struct page *page)
+{
+	if (cleancache_enabled && cleancache_fs_enabled(page))
+		__cleancache_put_page(page);
+}
+
+static inline void cleancache_flush_page(struct address_space *mapping,
+					struct page *page)
+{
+	/* careful... page->mapping is NULL sometimes when this is called */
+	if (cleancache_enabled && cleancache_fs_enabled_mapping(mapping))
+		__cleancache_flush_page(mapping, page);
+}
+
+static inline void cleancache_flush_inode(struct address_space *mapping)
+{
+	if (cleancache_enabled && cleancache_fs_enabled_mapping(mapping))
+		__cleancache_flush_inode(mapping);
+}
+
+static inline void cleancache_flush_fs(struct super_block *sb)
+{
+	if (cleancache_enabled)
+		__cleancache_flush_fs(sb);
+}
+
+#endif /* _LINUX_CLEANCACHE_H */
diff --git a/include/linux/fs.h b/include/linux/fs.h
index 32b38cd..973ce6d 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -1426,6 +1426,11 @@ struct super_block {
 	 */
 	char __rcu *s_options;
 	const struct dentry_operations *s_d_op; /* default d_op for dentries */
+
+	/*
+	 * Saved pool identifier for cleancache (-1 means none)
+	 */
+	int cleancache_poolid;
 };
 
 extern struct timespec current_fs_time(struct super_block *sb);
diff --git a/mm/Kconfig b/mm/Kconfig
index 3ad483b..4a81ef8 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -340,6 +340,28 @@ choice
 	  benefit.
 endchoice
 
+config CLEANCACHE
+	bool "Enable cleancache pseudo-RAM driver to cache clean pages"
+	default y
+	help
+ 	  Cleancache can be thought of as a page-granularity victim cache
+	  for clean pages that the kernel's pageframe replacement algorithm
+	  (PFRA) would like to keep around, but can't since there isn't enough
+	  memory.  So when the PFRA "evicts" a page, it first attempts to put
+	  it into a synchronous concurrency-safe page-oriented pseudo-RAM
+	  device (such as Xen's Transcendent Memory, aka "tmem") which is not
+	  directly accessible or addressable by the kernel and is of unknown
+	  (and possibly time-varying) size.  And when a cleancache-enabled
+	  filesystem wishes to access a page in a file on disk, it first
+	  checks cleancache to see if it already contains it; if it does,
+ 	  the page is copied into the kernel and a disk access is avoided.
+	  When a pseudo-RAM device is available, a significant I/O reduction
+	  may be achieved.  When none is available, all cleancache calls
+	  are reduced to a single pointer-compare-against-NULL resulting
+	  in a negligible performance hit.
+
+	  If unsure, say Y to enable cleancache
+
 #
 # UP and nommu archs use km based percpu allocator
 #
diff --git a/mm/Makefile b/mm/Makefile
index 2b1b575..0b569a4 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -43,3 +43,4 @@ obj-$(CONFIG_MEMORY_FAILURE) += memory-failure.o
 obj-$(CONFIG_HWPOISON_INJECT) += hwpoison-inject.o
 obj-$(CONFIG_DEBUG_KMEMLEAK) += kmemleak.o
 obj-$(CONFIG_DEBUG_KMEMLEAK_TEST) += kmemleak-test.o
+obj-$(CONFIG_CLEANCACHE) += cleancache.o
diff --git a/mm/cleancache.c b/mm/cleancache.c
new file mode 100644
index 0000000..f810608
--- /dev/null
+++ b/mm/cleancache.c
@@ -0,0 +1,258 @@
+/*
+ * Cleancache frontend
+ *
+ * This code provides the generic "frontend" layer to call a matching
+ * "backend" driver implementation of cleancache.  See
+ * Documentation/vm/cleancache.txt for more information.
+ *
+ * Copyright (C) 2009-2010 Oracle Corp. All rights reserved.
+ * Author: Dan Magenheimer
+ *
+ * This work is licensed under the terms of the GNU GPL, version 2.
+ */
+
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/exportfs.h>
+#include <linux/mm.h>
+#include <linux/cleancache.h>
+
+/*
+ * This global enablement flag may be read thousands of times per second
+ * by cleancache_get/put/flush even on systems where cleancache_ops
+ * is not claimed (e.g. cleancache is config'ed on but remains
+ * disabled), so is preferred to the slower alternative: a function
+ * call that checks a non-global.
+ */
+int cleancache_enabled;
+EXPORT_SYMBOL(cleancache_enabled);
+
+/*
+ * cleancache_ops is set by cleancache_ops_register to contain the pointers
+ * to the cleancache "backend" implementation functions.
+ */
+static struct cleancache_ops cleancache_ops;
+
+/* useful stats available in /sys/kernel/mm/cleancache */
+static unsigned long cleancache_succ_gets;
+static unsigned long cleancache_failed_gets;
+static unsigned long cleancache_puts;
+static unsigned long cleancache_flushes;
+
+/*
+ * register operations for cleancache, returning previous thus allowing
+ * detection of multiple backends and possible nesting
+ */
+struct cleancache_ops cleancache_register_ops(struct cleancache_ops *ops)
+{
+	struct cleancache_ops old = cleancache_ops;
+
+	cleancache_ops = *ops;
+	cleancache_enabled = 1;
+	return old;
+}
+EXPORT_SYMBOL(cleancache_register_ops);
+
+/* Called by a cleancache-enabled filesystem at time of mount */
+void __cleancache_init_fs(struct super_block *sb)
+{
+	sb->cleancache_poolid = (*cleancache_ops.init_fs)(PAGE_SIZE);
+}
+EXPORT_SYMBOL(__cleancache_init_fs);
+
+/* Called by a cleancache-enabled clustered filesystem at time of mount */
+void __cleancache_init_shared_fs(char *uuid, struct super_block *sb)
+{
+	sb->cleancache_poolid =
+		(*cleancache_ops.init_shared_fs)(uuid, PAGE_SIZE);
+}
+EXPORT_SYMBOL(__cleancache_init_shared_fs);
+
+/*
+ * If the filesystem uses exportable filehandles, use the filehandle as
+ * the key, else use the inode number.
+ */
+static int cleancache_get_key(struct inode *inode,
+			      struct cleancache_filekey *key)
+{
+	int (*fhfn)(struct dentry *, __u32 *fh, int *, int);
+	int maxlen = CLEANCACHE_KEY_MAX;
+	struct super_block *sb = inode->i_sb;
+	struct dentry *d;
+
+	key->u.ino = inode->i_ino;
+	if (sb->s_export_op != NULL) {
+		fhfn = sb->s_export_op->encode_fh;
+		if  (fhfn) {
+			d = list_first_entry(&inode->i_dentry,
+						struct dentry, d_alias);
+			(void)(*fhfn)(d, &key->u.fh[0], &maxlen, 0);
+			if (maxlen > CLEANCACHE_KEY_MAX)
+				return -1;
+		}
+	}
+	return 0;
+}
+
+/*
+ * "Get" data from cleancache associated with the poolid/inode/index
+ * that were specified when the data was put to cleanache and, if
+ * successful, use it to fill the specified page with data and return 0.
+ * The pageframe is unchanged and returns -1 if the get fails.
+ * Page must be locked by caller.
+ */
+int __cleancache_get_page(struct page *page)
+{
+	int ret = -1;
+	int pool_id;
+	struct cleancache_filekey key = { .u.key = { 0 } };
+
+	VM_BUG_ON(!PageLocked(page));
+	pool_id = page->mapping->host->i_sb->cleancache_poolid;
+	if (pool_id < 0)
+		goto out;
+
+	if (cleancache_get_key(page->mapping->host, &key) < 0)
+		goto out;
+
+	ret = (*cleancache_ops.get_page)(pool_id, key, page->index, page);
+	if (ret == 0)
+		cleancache_succ_gets++;
+	else
+		cleancache_failed_gets++;
+out:
+	return ret;
+}
+EXPORT_SYMBOL(__cleancache_get_page);
+
+/*
+ * "Put" data from a page to cleancache and associate it with the
+ * (previously-obtained per-filesystem) poolid and the page's,
+ * inode and page index.  Page must be locked.  Note that a put_page
+ * always "succeeds", though a subsequent get_page may succeed or fail.
+ */
+void __cleancache_put_page(struct page *page)
+{
+	int pool_id;
+	struct cleancache_filekey key = { .u.key = { 0 } };
+
+	VM_BUG_ON(!PageLocked(page));
+	pool_id = page->mapping->host->i_sb->cleancache_poolid;
+	if (pool_id >= 0 &&
+	      cleancache_get_key(page->mapping->host, &key) >= 0) {
+		(*cleancache_ops.put_page)(pool_id, key, page->index, page);
+		cleancache_puts++;
+	}
+}
+EXPORT_SYMBOL(__cleancache_put_page);
+
+/*
+ * Flush any data from cleancache associated with the poolid and the
+ * page's inode and page index so that a subsequent "get" will fail.
+ */
+void __cleancache_flush_page(struct address_space *mapping, struct page *page)
+{
+	/* careful... page->mapping is NULL sometimes when this is called */
+	int pool_id = mapping->host->i_sb->cleancache_poolid;
+	struct cleancache_filekey key = { .u.key = { 0 } };
+
+	if (pool_id >= 0) {
+		VM_BUG_ON(!PageLocked(page));
+		if (cleancache_get_key(mapping->host, &key) >= 0) {
+			(*cleancache_ops.flush_page)(pool_id, key, page->index);
+			cleancache_flushes++;
+		}
+	}
+}
+EXPORT_SYMBOL(__cleancache_flush_page);
+
+/*
+ * Flush all data from cleancache associated with the poolid and the
+ * mappings's inode so that all subsequent gets to this poolid/inode
+ * will fail.
+ */
+void __cleancache_flush_inode(struct address_space *mapping)
+{
+	int pool_id = mapping->host->i_sb->cleancache_poolid;
+	struct cleancache_filekey key = { .u.key = { 0 } };
+
+	if (pool_id >= 0 && cleancache_get_key(mapping->host, &key) >= 0)
+		(*cleancache_ops.flush_inode)(pool_id, key);
+}
+EXPORT_SYMBOL(__cleancache_flush_inode);
+
+/*
+ * Called by any cleancache-enabled filesystem at time of unmount;
+ * note that pool_id is surrendered and may be reutrned by a subsequent
+ * cleancache_init_fs or cleancache_init_shared_fs
+ */
+void __cleancache_flush_fs(struct super_block *sb)
+{
+	if (sb->cleancache_poolid >= 0) {
+		int old_poolid = sb->cleancache_poolid;
+		sb->cleancache_poolid = -1;
+		(*cleancache_ops.flush_fs)(old_poolid);
+	}
+}
+EXPORT_SYMBOL(__cleancache_flush_fs);
+
+#ifdef CONFIG_SYSFS
+
+/* see Documentation/ABI/xxx/sysfs-kernel-mm-cleancache */
+
+#define CLEANCACHE_ATTR_RO(_name) \
+	static struct kobj_attribute _name##_attr = __ATTR_RO(_name)
+
+static ssize_t cleancache_succ_gets_show(struct kobject *kobj,
+			       struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", cleancache_succ_gets);
+}
+CLEANCACHE_ATTR_RO(cleancache_succ_gets);
+
+static ssize_t cleancache_failed_gets_show(struct kobject *kobj,
+			       struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", cleancache_failed_gets);
+}
+CLEANCACHE_ATTR_RO(cleancache_failed_gets);
+
+static ssize_t cleancache_puts_show(struct kobject *kobj,
+			       struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", cleancache_puts);
+}
+CLEANCACHE_ATTR_RO(cleancache_puts);
+
+static ssize_t cleancache_flushes_show(struct kobject *kobj,
+			       struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", cleancache_flushes);
+}
+CLEANCACHE_ATTR_RO(cleancache_flushes);
+
+static struct attribute *cleancache_attrs[] = {
+	&cleancache_succ_gets_attr.attr,
+	&cleancache_failed_gets_attr.attr,
+	&cleancache_puts_attr.attr,
+	&cleancache_flushes_attr.attr,
+	NULL,
+};
+
+static struct attribute_group cleancache_attr_group = {
+	.attrs = cleancache_attrs,
+	.name = "cleancache",
+};
+
+#endif /* CONFIG_SYSFS */
+
+static int __init init_cleancache(void)
+{
+#ifdef CONFIG_SYSFS
+	int err;
+
+	err = sysfs_create_group(mm_kobj, &cleancache_attr_group);
+#endif /* CONFIG_SYSFS */
+	return 0;
+}
+module_init(init_cleancache)
diff --git a/mm/filemap.c b/mm/filemap.c
index 83a45d3..1f5a66f 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -34,6 +34,7 @@
 #include <linux/hardirq.h> /* for BUG_ON(!in_atomic()) only */
 #include <linux/memcontrol.h>
 #include <linux/mm_inline.h> /* for page_is_file_cache() */
+#include <linux/cleancache.h>
 #include "internal.h"
 
 /*
@@ -116,6 +117,16 @@ void __remove_from_page_cache(struct page *page)
 {
 	struct address_space *mapping = page->mapping;
 
+	/*
+	 * if we're uptodate, flush out into the cleancache, otherwise
+	 * invalidate any existing cleancache entries.  We can't leave
+	 * stale data around in the cleancache once our page is gone
+	 */
+	if (PageUptodate(page))
+		cleancache_put_page(page);
+	else
+		cleancache_flush_page(mapping, page);
+
 	radix_tree_delete(&mapping->page_tree, page->index);
 	page->mapping = NULL;
 	mapping->nrpages--;
diff --git a/mm/truncate.c b/mm/truncate.c
index 3c2d5dd..60c1ce6 100644
--- a/mm/truncate.c
+++ b/mm/truncate.c
@@ -19,6 +19,7 @@
 #include <linux/task_io_accounting_ops.h>
 #include <linux/buffer_head.h>	/* grr. try_to_release_page,
 				   do_invalidatepage */
+#include <linux/cleancache.h>
 #include "internal.h"
 
 
@@ -51,6 +52,7 @@ void do_invalidatepage(struct page *page, unsigned long offset)
 static inline void truncate_partial_page(struct page *page, unsigned partial)
 {
 	zero_user_segment(page, partial, PAGE_CACHE_SIZE);
+	cleancache_flush_page(page->mapping, page);
 	if (page_has_private(page))
 		do_invalidatepage(page, partial);
 }
@@ -108,6 +110,10 @@ truncate_complete_page(struct address_space *mapping, struct page *page)
 	clear_page_mlock(page);
 	remove_from_page_cache(page);
 	ClearPageMappedToDisk(page);
+	/* this must be after the remove_from_page_cache which
+	 * calls cleancache_put_page (and note page->mapping is now NULL)
+	 */
+	cleancache_flush_page(mapping, page);
 	page_cache_release(page);	/* pagecache ref */
 	return 0;
 }
@@ -215,6 +221,7 @@ void truncate_inode_pages_range(struct address_space *mapping,
 	pgoff_t next;
 	int i;
 
+	cleancache_flush_inode(mapping);
 	if (mapping->nrpages == 0)
 		return;
 
@@ -290,6 +297,7 @@ void truncate_inode_pages_range(struct address_space *mapping,
 		pagevec_release(&pvec);
 		mem_cgroup_uncharge_end();
 	}
+	cleancache_flush_inode(mapping);
 }
 EXPORT_SYMBOL(truncate_inode_pages_range);
 
@@ -432,6 +440,7 @@ int invalidate_inode_pages2_range(struct address_space *mapping,
 	int did_range_unmap = 0;
 	int wrapped = 0;
 
+	cleancache_flush_inode(mapping);
 	pagevec_init(&pvec, 0);
 	next = start;
 	while (next <= end && !wrapped &&
@@ -490,6 +499,7 @@ int invalidate_inode_pages2_range(struct address_space *mapping,
 		mem_cgroup_uncharge_end();
 		cond_resched();
 	}
+	cleancache_flush_inode(mapping);
 	return ret;
 }
 EXPORT_SYMBOL_GPL(invalidate_inode_pages2_range);
